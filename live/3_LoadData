dtu_catalog = dbutils.widgets.get("dtu_catalog")

query= f"""
CREATE OR REPLACE TABLE {dtu_catalog}.cnp4gen_silver.coolfarm_table AS 
SELECT * FROM read_files(
  '/Volumes/{dtu_catalog}/cnp4gen_silver/volume_coolfarm/250715_EXT_FARM_CONSOLIDATION_update.csv',
  format=> 'csv',
  header=> 'true',
  delimiter=> ';'
);"""

spark.sql(query)

# List contents of the volume after processing
dbutils.fs.ls('/Volumes/onehouse_eu_dev_dtu_cnpscos/cnpscos_silver/caps')

df = (
        spark.read.format("excel")
        .option("header", "true")
        .option("headerRows", 2)
        .option("inferSchema", "true")
        .option("dataAddress", "LogisticsCost")
        .load("/Volumes/onehouse_eu_dev_dtu_cnpscos/cnpscos_silver/compact_carton/Comp_Carton_LogisticsCost.xlsx")
    )

df.createOrReplaceTempView("logistics_cost")

result = spark.sql("""
    SELECT *
    FROM logistics_cost
""")
display(result)


%md
# Create View in Expo Schema

# Build the fully qualified table and view names
source_table = f"{dtu_catalog}.{dtu_schema}.{table_name}"
view_name = f"{dtu_catalog}.cnpscos_expo.{table_name}"

# Construct the SQL statement
sql_stmt = f"""
CREATE OR REPLACE VIEW {view_name}
AS SELECT * FROM {source_table}
"""

# Execute the SQL statement
spark.sql(sql_stmt)
