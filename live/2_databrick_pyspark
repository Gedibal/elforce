display(dbutils.fs.ls("/Volumes/onehouse_eu_dev_dtu_cnpscos/cnpscos_silver/compact_carton/"))

df = (
    spark.read.format("excel")
    .option("header", "true")
    .option("headerRows", 2)
    .option("dataAddress", sheet_name)
    .load(f"/Volumes/{dtu_catalog}/{dtu_schema}/compact_carton/{file_name}")
)

df = df.withColumnsRenamed(
    {
        "Divison": "division", 
        "SPP (â‚¬/1000pcs)": "spp_eur_per_1000pcs"
    }
)

df = df.withColumn(
    "division", expr(f"try_cast({"division"} as {"string"})")
)

# Speark read excel file
df = (
    spark.read.format("excel")
    .option("header", "true")
    .option("headerRows", 2)
    .option("inferSchema", "true")
    .option("dataAddress", sheet_name)
    .load( f"/Volumes/{dtu_catalog}/{dtu_schema}/compact_carton/{file_name}")
)

display(spark.sql("DESCRIBE EXTENDED {}".format(TABLE_CONFIGS[0]["target_table"])))
// TO list all columns name, data type and comment of columns in a table

dbutils.widgets.text("file_name", defaultValue="Comp_Carton_LogisticsCost.xlsx", label="File Name")
file_name = dbutils.widgets.get("file_name")


dbutils.widgets.text("dtu_catalog", defaultValue="onehouse_eu_dev_dtu_cnpscos", label="Catalog Name")
dbutils.widgets.text("dtu_schema", defaultValue="cnpscos_silver", label="Schema Name")

dtu_catalog = dbutils.widgets.get("dtu_catalog")
dtu_schema = dbutils.widgets.get("dtu_schema")

dbutils.secrets.list("platform-keyvault-managed")

client_secret = dbutils.secrets.get(scope="platform-keyvault-managed", key="dtuCNPSCOS-appRegistrationClientSecret-eu-d")

To list column name, data type and description
columns = spark.catalog.listColumns(
    tableName=target_table
)

df_columns = spark.createDataFrame([
    (col.name, col.dataType, col.description) for col in columns
], ["name", "data_type", "description"])

display(df_columns)

