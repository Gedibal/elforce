display(dbutils.fs.ls("/Volumes/onehouse_eu_dev_dtu_cnpscos/cnpscos_silver/compact_carton/"))

df = (
    spark.read.format("excel")
    .option("header", "true")
    .option("headerRows", 2)
    .option("dataAddress", sheet_name)
    .load(f"/Volumes/{dtu_catalog}/{dtu_schema}/compact_carton/{file_name}")
)

df = df.withColumnsRenamed(
    {
        "Divison": "division", 
        "SPP (â‚¬/1000pcs)": "spp_eur_per_1000pcs"
    }
)

df = df.withColumn(
    "division", expr(f"try_cast({"division"} as {"string"})")
)

# Speark read excel file
df = (
    spark.read.format("excel")
    .option("header", "true")
    .option("headerRows", 2)
    .option("inferSchema", "true")
    .option("dataAddress", sheet_name)
    .load( f"/Volumes/{dtu_catalog}/{dtu_schema}/compact_carton/{file_name}")
)

display(spark.sql("DESCRIBE EXTENDED {}".format(TABLE_CONFIGS[0]["target_table"])))
// TO list all columns name, data type and comment of columns in a table

dbutils.widgets.text("file_name", defaultValue="Comp_Carton_LogisticsCost.xlsx", label="File Name")
file_name = dbutils.widgets.get("file_name")
